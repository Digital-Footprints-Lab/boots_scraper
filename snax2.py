import sys
import time
import datetime
import requests
from bs4 import BeautifulSoup
import pandas as pd
from alive_progress import alive_bar
import targets # this will be the template in published version
import lxml      # alt to html.parser, with cchardet >> speed up
import cchardet  # character recognition


def get_links_from_one_category(category, baseurl):

    """Get the full URL for each product in a category

    Args: a category and baseURL from snax_tagets
    Rets: a pandas series of the product URLs"""

    page_number = 1
    product_links = []
    page_string = "?pageNo=" #~ the URL page counter
    #~ the final section of the category URL - often inconsistent
    category_name = category.split("/")[-1]

    with alive_bar(0, f"Acquiring product links for {category_name}") as bar:
        while True:
            #~ pull down the webpage
            target = requests.get(baseurl + category + page_string + str(page_number)).text
            #~ init BS object
            soup = BeautifulSoup(target, "lxml")
            #~ retrieve the link text element for all products on page
            product_list = soup.find_all("a", {"class": "product_name_link product_view_gtm"})
            #~ incrementing to an empty product page means we are done here
            if len(product_list) == 0:
                print(f"OK, {len(product_links)} {category_name} links retrieved [{page_number - 1} pages]")
                break
            #~ add to a list of the href URLs
            for product in product_list:
                link = product.get("href")
                product_links.append(link)
                bar() #~ increment progress bar
            #~ increment pagination
            page_number += 1
            #! sleep for a random length taken from the sleepy arg?
    #~ turn the list into a series and return
    linx = pd.Series(product_links)
    return linx


def make_dataframe_of_links_from_all_categories():
    all_links = pd.Series(dtype=str)
    print("\n" + f">>> Finding links for {len(targets.categories)} product categories:")
    for category in targets.categories:
        product_links = get_links_from_one_category(category, targets.baseurl)
        all_links = all_links.append(product_links, ignore_index=True)
    all_links = all_links.drop_duplicates().reset_index(drop=True)
    all_links = all_links.to_frame()
    all_links.columns = ["product_link"]
    all_links.to_csv("output/linx_" +
                     datetime.datetime.now().replace(microsecond=0).isoformat() +
                     ".csv")
    return all_links


def populate_links_df_with_extracted_fields(dataframe, fields_to_extract):

    """Takes a dataframe where column 0 = URLs, generated by
    the get links function. Puts the contents of each
    field of interest into a dataframe.

    Args: dataframe column 0 = a URL,
          list of fields (a list of lists)
    Rets: populated dataframe"""

    total_snax = len(fields_to_extract) * dataframe.shape[0]
    print("\n" + f">>> Requesting {total_snax} product details:")
    with alive_bar(total_snax,
                   f"""Acquiring {len(fields_to_extract)}
                   fields for {dataframe.shape[0]} products""") as bar:
        for index in range(dataframe.shape[0]):
            #~ pull down the full product page
            target = requests.get(dataframe.at[index, "product_link"]).text
            #~ init BSoup object
            soup = BeautifulSoup(target, "html.parser")
            for field in fields_to_extract:
                #~ use field identifiers to get values
                try:
                    if field[3] == "nested":
                        #~ this is tailored for the final field with "active ingredients" #! fix this!
                        try:
                            field_value = soup.find_all(field[0], attrs={field[1]: field[2]})[-1].get_text(strip=True) #find('h3', 'id'="product_active_ingredients").find('p').get_text(strip=True)
                        except IndexError:
                            print(f"Field \"{field[2]}\" not found")
                            continue
                    else:
                        field_value = soup.find(field[0], attrs={field[1]: field[2]}).get_text(strip=True) #'div', attrs={'class':'category5'}):
                except AttributeError:
                    print(f"Field \"{field[2]}\" not found")
                    continue
                dataframe.loc[index, field[2]] = field_value
                bar()
    dataframe.to_csv("output/snax_" +
                     datetime.datetime.now().replace(microsecond=0).isoformat() +
                     ".csv")
    return dataframe




def main():

    snax = make_dataframe_of_links_from_all_categories()

    #~ using links df, build new columns for each field
    try:
        snax = populate_links_df_with_extracted_fields(snax,
                                                       targets.fields_to_extract)
    except KeyboardInterrupt:
        print(snax)
        sys.exit(0)

    print(snax)

if __name__ == "__main__":
    main()

