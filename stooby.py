import requests
from bs4 import BeautifulSoup
import pandas as pd
from alive_progress import alive_bar
import time
import snax_targets2 # this will be the template in published version


def get_links_from_category(category, baseurl):

    """Get the full URL for each product in a category

    Takes: a category and baseURL from snax_tagets
    Returns: a pandas series of the product URLs"""

    page_number = 1
    product_links = []
    page_string = "?pageNo=" #~ the URL page counter
    #~ the final section of the category URL - often inconsistent
    category_name = category.split("/")[-1]

    with alive_bar(0, f"Acquiring product links for {category_name}") as bar:
        while True:
            #~ pull down the webpage
            target = requests.get(baseurl + category + page_string + str(page_number)).text
            #~ init BS object
            soup = BeautifulSoup(target, "html.parser")
            #~ retrieve the link text element for all products on page
            product_list = soup.find_all("a", {"class": "product_name_link product_view_gtm"})
            #~ incrementing to an empty product page means we are done here
            if len(product_list) == 0:
                print(f"OK, {len(product_links)} product links retrieved [{page_number - 1} pages]")
                break
            #~ add to a list of the href URLs
            for product in product_list:
                link = product.get("href")
                product_links.append(link)
                bar() #~ increment progress bar
            #~ increment pagination
            page_number += 1
    #~ turn the list into a series and return
    return pd.Series(product_links)

def extract_fields(product_link):

    """Takes a list of product URLs, generated by
    the get links function. Puts the contents of each
    field of interest into a dataframe.

    Takes: each URL, a list of fields
    Returns: populated dataframe"""

    #~ pull down the webpage
    target = requests.get(product_link).text
    #~ init BS object
    soup = BeautifulSoup(target, 'html.parser')
    #~ do some beautiful soup saucery
    #! turn this into a passed list
    field_value = soup.find("div", {"id": "productId"}).get_text()
    return field_value

def harvest_product_links():
    all_links = pd.Series(dtype=str)
    for category in snax_targets2.categories:
        product_links = get_links_from_category(category, snax_targets2.baseurl)
        all_links = all_links.append(product_links, ignore_index=True)
    all_links = all_links.drop_duplicates()
    all_links = all_links.to_frame()
    all_links.columns =["product_link"]
    return all_links

def main():
    #~ make a dataframe of all product links
    snax = harvest_product_links()

    #~ using that df, build new columns for each field
    with alive_bar(snax.shape[0],
                f"Acquiring fields for {snax.shape[0]} products",
                bar="smooth") as bar:
        for index in range(snax.shape[0]):
            link = snax.at[index, "product_link"]
            product_id = extract_fields(link)
            snax.loc[index, "product_id"] = product_id
            bar()
    print(snax)

if __name__ == "__main__":
    main()

#all_links.to_csv("links.csv")
#all_links.to_json("links.json")

